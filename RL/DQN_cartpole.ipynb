{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is a simple implementation of Deep Q network proposed by Mnih et.al 2015. We consider here the simple Cartpole game.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03766803 -0.18497433 -0.00271551  0.31925608] 1.0 False\n",
      "[-0.04136751 -0.3800575   0.00366962  0.61108141] 1.0 False\n",
      "[-0.04896866 -0.57523055  0.01589124  0.90491787] 1.0 False\n",
      "[-0.06047328 -0.38032737  0.0339896   0.61727185] 1.0 False\n",
      "[-0.06807982 -0.18569632  0.04633504  0.33548491] 1.0 False\n",
      "[-0.07179375  0.00873663  0.05304474  0.05776613] 1.0 False\n",
      "[-0.07161902 -0.18710418  0.05420006  0.366702  ] 1.0 False\n",
      "[-0.0753611  -0.38295275  0.0615341   0.67597048] 1.0 False\n",
      "[-0.08302016 -0.57887334  0.07505351  0.98737477] 1.0 False\n",
      "[-0.09459762 -0.77491566  0.094801    1.30265542] 1.0 False\n",
      "[-0.11009594 -0.58111551  0.12085411  1.04109029] 1.0 False\n",
      "[-0.12171825 -0.77761739  0.14167592  1.36913787] 1.0 False\n",
      "[-0.13727059 -0.58452367  0.16905868  1.12391279] 1.0 False\n",
      "[-0.14896107 -0.78140905  0.19153693  1.46449736] 1.0 False\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "env.reset()\n",
    "while True:\n",
    "    state,reward,done,_ = env.step(random.randint(0,1))\n",
    "    if done:\n",
    "        state = None\n",
    "        flag = 1\n",
    "    if state is not None:    \n",
    "        print(state,reward,done)\n",
    "    if flag==1:\n",
    "        break\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Generate state action samples..We generate beforehand a large number of samples so that we can sample from them later.\n",
    "# It is termed as the replay memory. Sampling from this set reduces the correlation between observations..\n",
    "# We would essentially a cyclic buffer of fixed size i.e., if the buffer is full and we would like to insert new observations we would remove the oldest memory to make way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self,capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self,transition):\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(transition)\n",
    "        else:\n",
    "            self.memory = self.memory[:-1]\n",
    "            self.memory = [transition] + self.memory\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def memoryLength(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cartpole game has two discrete actions. Our module which tries to estimate the Q-function takes the state as input\n",
    "# outputs estimates of Q(state, left) and Q(state, right)\n",
    "# we use two-layer feed forward network here...(this ofcourse is not the best choice but good enough for our understanding)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.i2h = nn.Linear(input_size,hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.i2h(input)\n",
    "        output = self.h2o(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = DQN(4,25,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = torch.from_numpy(state).float().view(1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = q_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07564590871334076"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "def select_action(q_net,state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return q_net(state).max(1)[1].item()\n",
    "    else:\n",
    "        return random.randrange(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_net = DQN(4,25,2)\n",
    "v_net.load_state_dict(q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)\n",
    "episode_durationss = []\n",
    "num_episodes = 1000\n",
    "for i_episode in range(num_episodes):\n",
    "    curr_state = env.reset()\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward])\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        c_s_tensor = torch.from_numpy(curr_state).float().view(1,4)\n",
    "        if next_state is not None:\n",
    "            n_s_tensor = torch.from_numpy(next_state).float().view(1,4)\n",
    "        else:\n",
    "            n_s_tensor = None\n",
    "        r_tensor = torch.from_numpy(reward).float().view(1,1)\n",
    "        memory.push(c_s_tensor, action, n_s_tensor, t_tensor)\n",
    "\n",
    "        # Move to the next state\n",
    "        curr_state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        v_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
